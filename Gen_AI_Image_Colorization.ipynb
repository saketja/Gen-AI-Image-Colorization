{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJifR61IFHKe0iMnqy93Iq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saketja/Gen-AI-Image-Colorization/blob/main/Gen_AI_Image_Colorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NAtdyvdW-koT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.current_device)\n",
        "else:\n",
        "  print(\"No NVIDIA driver found. Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So9Z29Hh_9BY",
        "outputId": "8022b93c-7bb2-45bb-cdda-c954615252fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NVIDIA driver found. Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True,num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False,num_workers=2)"
      ],
      "metadata": {
        "id": "rpe_6HE6Aqb6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorizationNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "    self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "    self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = nn.functional.relu(self.conv1(x))\n",
        "    x = nn.functional.relu(self.conv2(x))\n",
        "    x = nn.functional.relu(self.conv3(x))\n",
        "    x = torch.sigmoid(self.conv4(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "T5apiFP_KSFu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ColorizationNet().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def rgb_to_gray(img):\n",
        "  return img.mean(dim=1,keepdim=True)"
      ],
      "metadata": {
        "id": "9dbRib5SLQKp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "                for i,(images,_) in enumerate(train_loader):\n",
        "                    grayscale_images = rgb_to_gray(images).to(device)\n",
        "                    images = images.to(device)\n",
        "                    outputs = model(grayscale_images)\n",
        "                    loss = criterion(outputs, images)\n",
        "\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if i % 100 == 0:\n",
        "                        print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJVKDo8yMLvj",
        "outputId": "19a07f96-50cd-44c7-8655-1f9f4df90993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/782], Loss: 0.0232\n",
            "Epoch [1/30], Step [101/782], Loss: 0.0065\n",
            "Epoch [1/30], Step [201/782], Loss: 0.0063\n",
            "Epoch [1/30], Step [301/782], Loss: 0.0060\n",
            "Epoch [1/30], Step [401/782], Loss: 0.0049\n",
            "Epoch [1/30], Step [501/782], Loss: 0.0048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "  img = img/2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "\n",
        "  if len(img.shape)==2\n",
        "    plt.imshow(npimg, cmap=\"gray\")\n",
        "  else:\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
        "\n",
        "def visualize_all_three(original_images,grayscale_images,colorizwd_images,n=5):\n",
        "    fig = plt.figure(figsize=(3*n,4))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(1,3*n,3*i+1)\n",
        "        imshow(original_images[i])\n",
        "        ax.set_title(\"Original\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        ax = plt.subplot(1,3*n,3*i+2)\n",
        "        imshow(grayscale_images[i])\n",
        "        ax.set_title(\"Grayscale\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        ax = plt.subplot(1,3*n,3*i+3)\n",
        "        imshow(colorizwd_images[i])\n",
        "        ax.set_title(\"Colorized\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "quKwWYidQVy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_rgb_to_hsv(rgb):\n",
        "  r,g,b = rgb[:0,:,:], rgb[:,1,:,:],rgb[:,2,:,:]\n",
        "\n",
        "  max_val, _ = torch.max(rgb,dim=1)\n",
        "  min_val, _ = torch.min(rgb,dim=1)\n",
        "\n",
        "  h = torch.zero_like(r)\n",
        "  mask = (max_val==r)&(g>=b)\n",
        "  h[mask] = (g[mask]-b[mask])/diff[mask]\n",
        "  mask = (max_val==r)&(g<b)\n",
        "  h[mask] = (g[mask]-b[mask])/diff(mask) + 6.0\n",
        "  mask = (max_val==g)\n",
        "  h[mask] = (b[mask]-r[mask])/diff(mask) + 2.0\n",
        "  mask = (max_val==b)\n",
        "  h[mask] = (r[mask]-g[mask])/diff(mask) + 4.0\n",
        "  h = h / 6.0\n",
        "  h[diff == 0.0] = 0.0\n",
        "\n",
        "  s = torch.zeros_like(r)\n",
        "  s[diff != 0.0] = diff[diff != 0.0]/max_val[diff !=0.0]\n",
        "\n",
        "  v = max_val\n",
        "\n",
        "  return torch.stack([h,s,v],dim=1)"
      ],
      "metadata": {
        "id": "EtyssyyGQhfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, s, v = hsv[:, 0, :, :], hsv[:, 1, :, :], hsv[:, 2, :, :]\n",
        "    i = (h * 6.0).floor()\n",
        "    f = h * 6.0 - i\n",
        "    p = v * (1.0 - s)\n",
        "    q = v * (1.0 - s * f)\n",
        "    t = v * (1.0 - s * (1.0 - f))\n",
        "\n",
        "    i_mod = i % 6\n",
        "    r = torch.zeros_like(h)\n",
        "    g = torch.zeros_like(h)\n",
        "    b = torch.zeros_like(h)\n",
        "\n",
        "    r[i_mod == 0.0] = v[i_mod == 0.0]\n",
        "    g[i_mod == 0.0] = t[i_mod == 0.0]\n",
        "    b[i_mod == 0.0] = p[i_mod == 0.0]\n",
        "\n",
        "    r[i_mod == 1.0] = q[i_mod == 1.0]\n",
        "    g[i_mod == 1.0] = v[i_mod == 1.0]\n",
        "    b[i_mod == 1.0] = p[i_mod == 1.0]\n",
        "\n",
        "    r[i_mod == 2.0] = p[i_mod == 2.0]\n",
        "    g[i_mod == 2.0] = v[i_mod == 2.0]\n",
        "    b[i_mod == 2.0] = t[i_mod == 2.0]\n",
        "\n",
        "    r[i_mod == 3.0] = p[i_mod == 3.0]\n",
        "    g[i_mod == 3.0] = q[i_mod == 3.0]\n",
        "    b[i_mod == 3.0] = v[i_mod == 3.0]\n",
        "\n",
        "    r[i_mod == 4.0] = t[i_mod == 4.0]\n",
        "    g[i_mod == 4.0] = p[i_mod == 4.0]\n",
        "    b[i_mod == 4.0] = v[i_mod == 4.0]\n",
        "\n",
        "    r[i_mod == 5.0] = v[i_mod == 5.0]\n",
        "    g[i_mod == 5.0] = p[i_mod == 5.0]\n",
        "    b[i_mod == 5.0] = q[i_mod == 5.0]\n",
        "\n",
        "    return torch.stack([r, g, b], dim=1)"
      ],
      "metadata": {
        "id": "x2vCLYTtUD3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exaggerate_colors(images.saturation_factor=1.5,value_factor=1.2):\n",
        "  images = (images + 1) / 2.0\n",
        "\n",
        "    # Convert RGB images to HSV\n",
        "    images_hsv = torch_rgb_to_hsv(images)\n",
        "\n",
        "    # Increase the saturation and value components\n",
        "    images_hsv[:, 1, :, :] = torch.clamp(images_hsv[:, 1, :, :] * saturation_factor, 0, 1)\n",
        "    images_hsv[:, 2, :, :] = torch.clamp(images_hsv[:, 2, :, :] * value_factor, 0, 1)\n",
        "\n",
        "    # Convert the modified HSV images back to RGB\n",
        "    color_exaggerated_images = torch_hsv_to_rgb(images_hsv)\n",
        "\n",
        "    # Convert images back to the range [-1, 1]\n",
        "    color_exaggerated_images = color_exaggerated_images * 2.0 - 1.0\n",
        "\n",
        "    return color_exaggerated_images"
      ],
      "metadata": {
        "id": "NTDCPVa8U7GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        colorized_images = model(grayscale_images)\n",
        "\n",
        "\n",
        "        grayscale_images_cpu = grayscale_images.cpu().squeeze(1)\n",
        "        colorized_images_cpu = colorized_images.cpu()\n",
        "        original_images_cpu = images.cpu()\n",
        "\n",
        "        #colorized_images_cpu=scale_predicted_colors(colorized_images_cpu)\n",
        "        colorized_images_cpu=exaggerate_colors(colorized_images_cpu)\n",
        "\n",
        "        # Visualize the grayscale, colorized, and original images\n",
        "        visualize_all_three(original_images_cpu, grayscale_images_cpu, colorized_images_cpu)\n",
        "\n",
        "        if i == 10:  # only do this for up to certain batch for demonstration purposes\n",
        "            break"
      ],
      "metadata": {
        "id": "nq75KJodVExg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory:\", current_directory)"
      ],
      "metadata": {
        "id": "RtAYarmkW1zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List the uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(\"Uploaded file:\", filename)"
      ],
      "metadata": {
        "id": "mtNzy-VJW2X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in the current directory\n",
        "files = os.listdir(\"/content\")\n",
        "print(\"Files in the current directory:\", files)"
      ],
      "metadata": {
        "id": "NoMHvLxLXZxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open the image. (Keep your image in the current directory. In my case, the image was horse.jpg)\n",
        "img = Image.open('eiffiltower.jpeg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray_img = img.convert(\"L\")"
      ],
      "metadata": {
        "id": "A_Zsdvo9XbL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # If you need to normalize, uncomment the following line\n",
        "    # transforms.Normalize(mean=[0.5], std=[0.5])  # Assuming you want to normalize to [-1, 1] range\n",
        "])"
      ],
      "metadata": {
        "id": "gombsE_NXr7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the transformations\n",
        "img_tensor = transform(gray_img).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the image tensor to the device where your model is (likely 'cuda' if using GPU)\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Get the model's output\n",
        "with torch.no_grad():\n",
        "    colorized_tensor = model(img_tensor)"
      ],
      "metadata": {
        "id": "i3qJj-G5XuRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the tensor back to an image\n",
        "colorized_img = transforms.ToPILImage()(colorized_tensor.squeeze(0).cpu())\n",
        "\n",
        "# Optionally, save the image\n",
        "colorized_img.save(\"_colorized2.jpg\")"
      ],
      "metadata": {
        "id": "IrUpsSX9XwdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the original, grayscale, and colorized images side-by-side\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Create a figure with 1 row and 3 columns\n",
        "\n",
        "# Display original color image\n",
        "ax[0].imshow(img)\n",
        "ax[0].set_title(\"Original Color Image\")\n",
        "ax[0].axis('off')  # Hide axes\n",
        "\n",
        "# Display grayscale image\n",
        "ax[1].imshow(gray_img, cmap='gray')  # Since it's grayscale, use cmap='gray'\n",
        "ax[1].set_title(\"Grayscale Image\")\n",
        "ax[1].axis('off')  # Hide axes\n",
        "\n",
        "# Display colorized image\n",
        "ax[2].imshow(colorized_img)\n",
        "ax[2].set_title(\"Colorized Image\")\n",
        "ax[2].axis('off')  # Hide axes\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UbS6PrBHXzYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}